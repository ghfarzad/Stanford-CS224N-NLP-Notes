# Word2vec
Word2vec is an NLP technique that uses a neural network model to learn word associations from a large corpus of text. The model is a shallow 2-layer model that takes a large corpus of text as input and generates a vector space as output. Each unique word in the corpus is represented by a vector in the vector space. This technique can be applied in two ways:
1. Continuous Baf od Words (CBOW) -> predicts current word from surrounding context
1. Continuous Skep-Gram -> used current word to predict surrounding context

Local representation of a word is a vector of only zeros and ones where only one element can be one.
Distributed representation of a word is a vector whose elements are all non zeros. In this represenation the meaning of the word is distrubuted across all elements of the vector.

Minimize Objectve function <=> Maximize Prodictive Accuracy
  - Stick a minus sign to do minimization rather than maximization
  - Stick 1/T (T being the size of the corpus) in the begining to make the problem independent of the size of the corpus.
  - Take the log because it makes easier when dealing with products
  - Dot product gives similarity between two vectors
  - Exp make everything positive
  
$$
R_{\mu \nu} - {1 \over 2}g_{\mu \nu}\,R + g_{\mu \nu} \Lambda
= {8 \pi G \over c^4} T_{\mu \nu}
$$
